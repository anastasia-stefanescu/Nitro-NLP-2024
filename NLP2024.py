# -*- coding: utf-8 -*-
"""secundar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MmQ2azXtum4eKfmzlbxzsRHrzH0KllxQ
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

train_df = pd.read_csv('/content/drive/MyDrive/train.csv')
test_df = pd.read_csv('/content/drive/MyDrive/test.csv')

!pip install -q transformers pytorch_lightning

import torch

import os
import sys
import json
import logging
import random

import numpy as np

import torch.nn as nn
import pytorch_lightning as pl


from pprint import pprint

from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset
from pytorch_lightning import LightningModule, Trainer
from pytorch_lightning.callbacks import EarlyStopping
from transformers import AutoTokenizer, AutoModel, AutoConfig, TrainingArguments

#de pus train - de impartit
# train_df = pd.read_csv('train.csv')
# test_df = pd.read_csv('test.csv')

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ensure reproducibility
seed_value = 42
torch.manual_seed(seed_value)
np.random.seed(seed_value)

MODEL_PATH = "dumitrescustefan/bert-base-romanian-cased-v1"

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, strip_accents=False)

import nltk
nltk.download('punkt')  # Download the Punkt tokenizer models

from nltk.tokenize import sent_tokenize

from sklearn.metrics import accuracy_score

train_df.fillna(' ')

import re

#Split text to tokens, i split only by spaces

# def split_to_tokens(text):
#   return re.split(" ",text)

#Replace emoticons and emojis

# import emoji

# replacements = {
#     r':\)\)+' : "raseste",
#     r':\)\)'  : "ras",
#     r':\)'    : "zambet",
#     r':D'     : "zambet",
#     r':-\)'   : "zambet",
#     r':p'     : "limba scoasa",
#     r'\(y\)'  : "ador"
# }

# def convert_emoticons_emoji(text, replacements):
#   result = text
#   for regex_condition, replacement in replacements.items():
#       result = re.sub(regex_condition, replacement, result)

#   return emoji.demojize(result)

# from num2words import num2words

# def numbers_to_words(text):
#   def replace(match):
#       number = int(match.group(0))
#       return num2words(number)

  # Use regex to find all numbers in the text
  #and replace them with ""
def remove_numbers(text):
  pattern = r'\b\d+\b'
  result = re.sub(pattern, "", text)
  return result

def remove_links(text):
  # Regular expression pattern to match URLs
  pattern = r'http\S+'

  # Use re.sub() to remove all matches of the pattern from the text
  result = re.sub(pattern, '', text)

  return result

def remove_hashtags(text):
  # Regular expression pattern to match hashtags
  pattern = r'#\w+'

  # Use re.sub() to remove all matches of the pattern from the text
  result = re.sub(pattern, '', text)

  return result

def remove_references(text):
  # Regular expression pattern to match references (mentions)
  pattern = r'@\w+'

  # Use re.sub() to remove all matches of the pattern from the text
  result = re.sub(pattern, '', text)

  return result

def remove_dupp_chr(text):
  result = []
  for i in range(len(text)):
    #verifica urmatorul caracter e identic
    if len(result)>0:
      if text[i] != result[-1]:
        result.append(text[i])
    else:
      result.append(text[i])
  return "".join(result)

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')


#sa folosim??
def remove_stopwords(text):
  # Tokenize the text into words
  words = text.split()

  # Get the list of English stopwords
  stop_words = set(stopwords.words('romanian'))

  # Filter out stopwords from the text
  filtered_words = [word for word in words if word.lower() not in stop_words]

  # Join the filtered words back into a single string
  filtered_text = ' '.join(filtered_words)

  return filtered_text

#nu avem nevoie
def convert_to_lowercase(text):
    return text.lower()

def preprocessing(text):
  # Scrie o funcÈ›ie care primeÈ™te un text È™i returneazÄƒ varianta preprocesatÄƒ.
  #FuncÈ›ia va converti toate numerele Ã®n cuvinte folosind num2words, va elimina linkurile, hashtagurile, mentions, punctuaÈ›ia, stopwords, va face toate textele literÄƒ micÄƒ È™i va aplica lematizarea sau stemming.
  result = text
  result = remove_numbers(result)
  result = remove_links(result)
  result = remove_hashtags(result)
  result = remove_references(result)
  #result = remove_punctuation(result)
  result = remove_dupp_chr(result)
  result = remove_stopwords(result)
  #result = convert_to_lowercase(result)

  #sa punem si lematizare?
  #result = lemmatize_word(result)
  return result


train_df['title'] = train_df['title'].astype(str).apply(preprocessing)
train_df['content'] = train_df['content'].astype(str).apply(preprocessing)

class CustomDataset(Dataset):
    """
    Custom dataset for processing text data, including special tokens for BERT models.
    """
    def __init__(self, df):
        df['content'] = df['content'].fillna('')
        df['content2'] = df['title'] + ' ' + df['content']
        df['content2'] = df['content2'].astype(str)
        self.samples = []

        for id, row in df.iterrows():
          text = row['content2']
          standardized_text = self._standardize_diacritics(text)
          formatted_text = self._format_with_special_tokens(standardized_text)

          sample = ({"content" : formatted_text, "class" : row['class']})
          self.samples.append(sample)

    def _standardize_diacritics(self, text):
        return text.replace("ţ", "ț").replace("ş", "ș").replace("Ţ", "Ț").replace("Ş", "Ș")

    def _format_with_special_tokens(self, text):
        """Format sentences with [CLS] and [SEP] tokens. """
        sentences = sent_tokenize(text)
        joined_text = "[CLS]" + "[SEP]".join(sentences)
        return joined_text

    #am pus sa returneze self.samples ca sa nu intre in recursie infinita
    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        return self.samples[index]

train_dataset = CustomDataset(train_df)

y = train_df['class']

from sklearn.model_selection import train_test_split
import sys

sys.setrecursionlimit(10000)

df_train, df_val, y_train, y_val = train_test_split(train_df, y, test_size = 0.3, random_state = 42)
df_val, df_test, y_val, y_test = train_test_split(df_val, y_val, test_size = 0.5, random_state = 42)

# # Split the data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(train_dataset, y, test_size=0.3, random_state = 42)

# # And we finally split the testing data into test/valid
# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state = 42)

train_dataset = CustomDataset(df_train)
val_dataset = CustomDataset(df_val)
test_dataset = CustomDataset(df_test)

class CustomCollator:
    """
    A custom data collator that prepares batches for model training or evaluation.
    This collator handles tokenization and ensures that sequences are padded to a uniform length.
    """
    def __init__(self, tokenizer: AutoTokenizer, max_seq_len: int):
        """
        Initializes the collator with a tokenizer and maximum sequence length.
        """
        self.max_seq_len = max_seq_len
        self.tokenizer = tokenizer

    def __call__(self, input_batch: list[dict]) -> dict:
        """
        Processes a batch of input data, tokenizing sequences and padding them to the same length.

        :param input_batch: A list of dictionaries, where each dictionary contains 'class' key.
        :return: A dictionary with tokenized and padded sequences, and associated class.
        """
        classes = [instance['class'] for instance in input_batch]
        texts = [instance['content'] for instance in input_batch]

        tokenized_batch = self.tokenizer(
            texts,
            padding=True,
            max_length=self.max_seq_len,
            truncation=True,
            return_tensors="pt"
        )
        classes_tensor = torch.tensor(classes, dtype=torch.float)

        return {
            "tokenized_batch": tokenized_batch,
            "classes": classes_tensor
        }

MAX_SEQ_LEN = 256  # Adjust the maximum length of the sequence to your needs
BATCH_SIZE = 16    # Adjust the batch size to your requirements

custom_collator = CustomCollator(tokenizer, MAX_SEQ_LEN)

#aici a trebuit sa setez shuffle = False ca sa mearga

train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collator, num_workers = 2, pin_memory=True)

validation_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collator, num_workers = 2, pin_memory=True)
test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collator, num_workers = 2)

class RoBERTModel(pl.LightningModule):
  def __init__(self, model_name: str, lr: float = 2e-5, sequence_max_length: int = MAX_SEQ_LEN):
    """
    Initializes the RoBERTModel with a specified base model and configuration.

    :param model_name: Name or path to the pretrained model.
    :param lr: Learning rate.
    :param sequence_max_length: Maximum input sequence length for the model.
    """
    super().__init__()

    self.tokenizer = AutoTokenizer.from_pretrained(model_name, strip_accents=False)
    self.model = AutoModel.from_pretrained(model_name)
    self.output_layer = nn.Linear(self.model.config.hidden_size, 1)

    self.loss_fct = nn.MSELoss()

    self.lr = lr
    self.save_hyperparameters()

    self.gpu_available = torch.cuda.is_available()
    if self.gpu_available:
      print(f"GPU is available: {torch.cuda.get_device_name(0)}")
    else:
      print("GPU is not available, using CPU.")

  def forward(self, tokenized_batch):
    """
    Forward pass through the model.

    :param tokenized_batch: Tokenized input batch including input_ids and attention_mask.
    :return: Predictions for the input batch.
    """
    output = self.model(**tokenized_batch, return_dict=True)
    cls_embedding = output.pooler_output
    prediction = self.output_layer(cls_embedding)

    return prediction.flatten()


  def training_step(self, batch, batch_idx):
    tokenized_batch = batch['tokenized_batch']
    ground_truth = batch['classes'].float() #aici am convertit la float

    prediction = self.forward(tokenized_batch)
    loss = self.loss_fct(prediction, ground_truth)

    ## For CPU: Conditionally detach and move the loss to CPU if GPU is available, and convert to
    # TODO
    # if self.gpu_available:
    #     loss = loss.detach().cpu()
    # else:
    #     loss = loss.detach()
    if not self.gpu_available:
        loss = loss.detach()

    binary_predictions = (prediction > 0.5).float()
    accuracy = accuracy_score(ground_truth.cpu().numpy(), binary_predictions.detach().cpu().numpy())

    self.log("train_loss", loss.detach().cpu().item(), on_step=True, on_epoch=True, prog_bar=True)
    self.log("train_accuracy", accuracy, on_step = True, on_epoch = True, prog_bar = True)
    return {"loss": loss}


  def validation_step(self, batch, batch_idx):
    tokenized_batch = batch['tokenized_batch']
    ground_truth = batch['classes'].float()

    prediction = self.forward(tokenized_batch)
    loss = self.loss_fct(prediction, ground_truth)

    ## For CPU: Conditionally detach and move the loss to CPU if GPU is available, and convert to
    # TODO
    if not self.gpu_available:
        loss = loss.detach()

    binary_predictions = (prediction > 0.5).float()
    accuracy = accuracy_score(ground_truth.cpu().numpy(), binary_predictions.detach().cpu().numpy())

    self.log("val_loss", loss.detach().cpu().item(), on_step=False, on_epoch=True, prog_bar = True)
    self.log("val_accuracy", accuracy, on_step=False, on_epoch=True, prog_bar = True)
    return {"loss": loss}

  def configure_optimizers(self):
    """
    Configures the model's optimizers.

    :return: The AdamW optimizer with the defined learning rate and parameters.
    """
    return AdamW([p for p in self.parameters() if p.requires_grad], lr=self.lr, eps=1e-08)

model = RoBERTModel(MODEL_PATH)

trainer = pl.Trainer(
    devices=-1,  # Comment this when training on cpu
    accelerator="gpu",
    max_epochs=-1,  # Set this to -1 when training fully
    limit_train_batches=10,  # Uncomment this when training fully
    limit_val_batches=5,  # Uncomment this when training fully
    gradient_clip_val=1.0,
    enable_checkpointing=True,
    #aici am adaugat
    log_every_n_steps = 10
)

#TRAINing
trainer.fit(model, train_dataloader, validation_dataloader)

# After training:
# Retrieve the final validation loss and accuracy from trainer.callback_metrics
final_val_loss = trainer.callback_metrics['val_loss']
final_val_accuracy = trainer.callback_metrics['val_accuracy']

# Retrieve the final training loss and accuracy from trainer.callback_metrics
final_train_loss = trainer.callback_metrics['train_loss']
final_train_accuracy = trainer.callback_metrics['train_accuracy']

print("Final Validation Loss:", final_val_loss)
print("Final Validation Accuracy:", final_val_accuracy)

print("Final Training Loss:", final_train_loss)
print("Final Training Accuracy:", final_train_accuracy)

def predict(model, title, content):
    """
    Generates a prediction score for a pair of sentences using a given model.
        float: The prediction score, scaled by a factor of 5.
    """
    # Format the input sentences for the model. Prepend [CLS] and append [SEP] as needed.
    text = str(title) + (str(content))
    text = text.replace("ţ", "ț").replace("ş", "ș").replace("Ţ", "Ț").replace("Ş", "Ș")
    text = preprocessing(text)
    sentences = sent_tokenize(text)
    joined_text = "[CLS]" + "[SEP]".join(sentences)

    # Tokenize the concatenated sentences. Specify padding (True), max_length (MAX_SEQ_LEN), truncation (True), and tensor type (pt).
    tokenized_batch = model.tokenizer(
        joined_text,
        padding=True,
        max_length=MAX_SEQ_LEN,
        truncation=True,
        return_tensors="pt"
    )

    # Move the tokenized_batch to the same device as the model's parameters
    tokenized_batch = {key: value.to(next(model.parameters()).device) for key, value in tokenized_batch.items()}

    # Generate predictions using the model's forward method. No need to explicitly call forward.
    predictions = model(tokenized_batch)

    # The prediction is scaled by 2 due to normalization applied during the model's training phase.
    # This scaling ensures the prediction is on the same scale as the target values.
    prediction_score = 1 if predictions[0].item() >= 0.5 else 0

    return prediction_score

def evaluate_model_on_tests(model, lista, output_file):
    """
    Evaluate the model on a series of tests and print their similarity scores.

    Args:
        model: The trained model to be evaluated.
        tests: A list of title+content
    """
    model.eval()  # Set the model to evaluation mode.

    device = next(model.parameters()).device  # Get the device of the model's parameters.

    with torch.no_grad(), open(output_file, 'w') as file:  # Inference mode, no gradients needed.
        lung = len(lista)
        for i in range(0, lung):
            title = lista[i][0]
            content = lista[i][1]

            rez = predict(model, title, content)
            file.write(f"{i} {rez}\n")

test_df.fillna(' ')
data_test = list(test_df.to_records(index=False))
data_test_str = [(str(elem[1]), str(elem[2])) for elem in data_test]

# Call the function to evaluate the model on the test data.
output_file = "predictions.txt"
evaluate_model_on_tests(model, data_test_str, output_file)